{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "SjTML4IQQdOV",
    "outputId": "ace99e88-e239-4182-c2dd-cdb9cc934f0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dominic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "# !pip3 install nest_asyncio\n",
    "# !pip3 install twint\n",
    "# !pip3 install nltk\n",
    "#pip install tweet-preprocessor\n",
    "import nest_asyncio\n",
    "import string\n",
    "from datetime import datetime\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pytz\n",
    "import textblob\n",
    "import json\n",
    "import itertools\n",
    "from elasticsearch import Elasticsearch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer, WhitespaceTokenizer\n",
    "import preprocessor as p\n",
    "\n",
    "#CORPORA\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaMulticore\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "\n",
    "#PYLDAVIS\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "nltk.download('stopwords')\n",
    "#%run \"candidates.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBVMuwlG7ZDK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:37: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:37: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-14-1d660f054867>:37: DeprecationWarning: invalid escape sequence \\s\n",
      "  word_tokens = RegexpTokenizer('\\s+', gaps=True).tokenize(newdoc)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TwitterClient:\n",
    "    '''\n",
    "    Twitter Twint intialization\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        #Intializes the Twint object\n",
    "        self.twint_Api = twint.Config()\n",
    "\n",
    "    def preprocess(self, tweets):\n",
    "        for i in range(len(tweets)):\n",
    "            tweets.at[i, 'tweet'] = p.clean( tweets.at[i, 'tweet']) \n",
    "        return tweets\n",
    "    \n",
    "    def clean_tweets(self, data ):\n",
    "        cleaned_tweets = {}\n",
    "        newdata = []\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = set(nltk.corpus.words.words())\n",
    "\n",
    "        for token in data.tweet: \n",
    "            newdoc = \"\"\n",
    "            #print(token)\n",
    "            seperator = \" \" \n",
    "            token = re.sub('[^A-Za-z0-9]+', ' ', token) #remove special characters\n",
    "            token = re.sub(r'\\d+', '', token) #remove numbers\n",
    "            token = re.sub(r'\\b\\w{1,2}\\b', '', token) #remove words with <= 2 characters\n",
    "            token = \" \".join(w for w in nltk.wordpunct_tokenize(token) \\\n",
    "                    # if w.lower() in words or \n",
    "                     or not w.isalpha())\n",
    "            whitespace_token = WhitespaceTokenizer().tokenize( token )\n",
    "            wo_stopwords_token = [x for x in whitespace_token\n",
    "                                      if not x in stop_words]\n",
    "            newdoc = seperator.join( (wo_stopwords_token) ).lower()\n",
    "\n",
    "            #create word tokens for each tweet\n",
    "            word_tokens = RegexpTokenizer('\\s+', gaps=True).tokenize(newdoc)\n",
    "\n",
    "            #data[idx] = newdoc\n",
    "            newdata.append( word_tokens )\n",
    "\n",
    "        return newdata\n",
    "\n",
    "    def get_tweet_sentiment(self, tweet):\n",
    "        analysis = TextBlob(tweet)\n",
    "        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n",
    "\n",
    "    def twint_to_pandas(self, col):\n",
    "        return twint.output.panda.Tweets_df[col]\n",
    "\n",
    "    def get_tweets(self, count):\n",
    "\n",
    "        #Creates a search string from the list of presidential candidates\n",
    "        self.twint_Api.Username = '@realDonaldTrump'\n",
    "        self.twint_Api.Limit = count\n",
    "        self.twint_Api.Format = \"Date: {date} | Tweet: {tweet}\"\n",
    "        self.twint_Api.Location = True\n",
    "        self.twint_Api.Lang = \"en\"\n",
    "        self.twint_Api.Pandas = True\n",
    "        #self.Output = \"output.csv\"\n",
    "        #self.twint_Api.Popular_tweets = True\n",
    "        #self.twint_Api.Pandas_au = True\n",
    "        #self.twint_Api.Timedelta = 340\n",
    "        tweets_df = pd.DataFrame(columns=[\"date\", \"tweet\"])\n",
    "        self.twint_Api.Hide_output = True\n",
    "        \n",
    "        twint.run.Search(self.twint_Api)\n",
    "        \n",
    "        tweets_df = tweets_df.append(self.twint_to_pandas([\"date\", \"tweet\"]), ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        tweets_df = self.preprocess(tweets_df)\n",
    "        #print(tweets_df)\n",
    "        toks = tweets_df = self.clean_tweets(tweets_df)\n",
    "        #tweets_df\n",
    "        \n",
    "        return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bow_lda_model(data):\n",
    "    l_dictionary = Dictionary(data)\n",
    "    print(\"Dictionary Loaded\")\n",
    "    bow_corpus = [l_dictionary.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=l_dictionary,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "        \n",
    "    vocab = list(l_dictionary.values()) #list of terms in the dictionary\n",
    "    \n",
    "    vocab_tf = [dict(i) for i in bow_corpus]\n",
    "    vocab_tf = list(pd.DataFrame(vocab_tf).sum(axis=0)) #list of term frequencies\n",
    "    \n",
    "    #bow = pd.DataFrame(vocab_tf, index=vocab, columns=[\"term_freq\"])\n",
    "    \n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary=l_dictionary)\n",
    "\n",
    "    pyLDAvis.save_html(vis, 'lda.html')\n",
    "    \n",
    "    bow = dict(zip(vocab, vocab_tf))\n",
    "    #bow.to_dict('index')\n",
    "   # print( bow.keys() )\n",
    "        \n",
    "    top_terms = []\n",
    "    for j in range(5):\n",
    "        c_terms = []\n",
    "        for i in range(len(lda_model.show_topic(j))):\n",
    "            c_terms.append(lda_model.show_topic(j)[i][0] )\n",
    "        top_terms.append(c_terms)\n",
    "        \n",
    "    print( top_terms )\n",
    "    \n",
    "    final_list = {\n",
    "                    \"name\": \"Donald Trump\",\n",
    "                    \"children\": []\n",
    "    }\n",
    "    \n",
    "    for l in range(len(top_terms)):\n",
    "        c_topic = { \n",
    "                        \"name\": str(l),\n",
    "                        \"children\": []\n",
    "                      }\n",
    "        for m in range(len(top_terms[l])):\n",
    "            c_topic[\"children\"].append({ \"name\": top_terms[l][m],\n",
    "                                         \"size\": bow[ top_terms[l][m] ] })\n",
    "            \n",
    "        final_list[\"children\"].append( c_topic )\n",
    "    \n",
    "    #print( final_list )\n",
    "    \n",
    "    with open('json/trump.json', 'w') as json_file:\n",
    "        json.dump(final_list, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "afQ3-uIWQ4WX",
    "outputId": "cf5da96b-7daa-4b51-cd5e-8d4a90828259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominic/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nothing', 'get', 'they', 'country', 'states', 'united', 'said', 'ukraine', 'time', 'ukrainian'], ['republican', 'election', 'thank', 'state', 'must', 'party', 'usa', 'pro', 'john', 'biden'], ['schiff', 'news', 'whistleblower', 'even', 'congress', 'got', 'adam', 'fake', 'hunt', 'media'], ['president', 'statement', 'turkey', 'good', 'all', 'would', 'making', 'tough', 'isis', 'fighting'], ['the', 'great', 'democrats', 'this', 'trump', 'impeachment', 'many', 'border', 'going', 'job']]\n"
     ]
    }
   ],
   "source": [
    "d = TwitterClient()\n",
    "tweets = d.get_tweets(1000)\n",
    "\n",
    "bow_lda_model( tweets )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXU2hNdg-RMo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FinalProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
